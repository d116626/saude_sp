{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from google.cloud import bigquery\n",
    "# from google.cloud import storage\n",
    "\n",
    "\n",
    "def save_in_storage(local_file_name, storage_file_name, local_path_to_file, storage_paht_to_save):\n",
    "    \"\"\"\n",
    "    local_file_name: nome da tabela local\n",
    "    storage_file_name: nome da tabela que sera criada no storage\n",
    "    local_path_to_file: caminho para a pasta local onde se encontra a tabela\n",
    "    storage_paht_to_save: nome da pasta onde os dados serão salvos no storage    \n",
    "    \"\"\"\n",
    "    file_path_csv = \"{}/{}.csv\".format(local_path_to_file,local_file_name)\n",
    "    file_path_parquet = \"{}/{}.parquet\".format(local_path_to_file,local_file_name)\n",
    "    pd.read_csv(file_path_csv).to_parquet(file_path_parquet, index=False)\n",
    "    storage_file_name_parquet = \"{}.parquet\".format(storage_file_name)\n",
    "    \n",
    "    client = storage.Client(project='gabinete-sv')\n",
    "    bucket = client.get_bucket('gabinete-sv')\n",
    "              \n",
    "    blob = bucket.blob('{}/{}'.format(storage_paht_to_save,storage_file_name_parquet))\n",
    "    blob.upload_from_filename(file_path_parquet)\n",
    "    \n",
    "\n",
    "\n",
    "def create_in_bquery(sorage_file_name,bquery_table_name,conjunto_de_dados,sorage_paht_to_file):\n",
    "    \"\"\"\n",
    "    bquery_table_name: nome da tabela que sera criada no bigquery\n",
    "    sorage_file_name: nome da tabela do storage\n",
    "    conjunto de dados: nome do schema onde sera salvo a tabela\n",
    "    sorage_paht_to_file: nome da pasta onde se encontra os dados no storage    \n",
    "    \"\"\"\n",
    "\n",
    "    client = bigquery.Client( project='gabinete-sv')\n",
    "    \n",
    "    dataset_ref = client.dataset(conjunto_de_dados)\n",
    "    \n",
    "    #nome da tabela que será criada no big query\n",
    "    table_name = dataset_ref.table(bquery_table_name)\n",
    "    \n",
    "    job_config = bigquery.LoadJobConfig()\n",
    "    #overwite table\n",
    "    job_config.write_disposition = \"WRITE_TRUNCATE\"\n",
    "    #source format\n",
    "    job_config.source_format = bigquery.SourceFormat.PARQUET\n",
    "    \n",
    "    \n",
    "    uri = \"gs://gabinete-sv/{}/{}.parquet\".format(sorage_paht_to_file,sorage_file_name)\n",
    "        \n",
    "    \n",
    "    load_job = client.load_table_from_uri(uri, table_name, job_config=job_config)  # API request\n",
    "    \n",
    "    print(\"Starting job {}\".format(load_job.job_id))\n",
    "\n",
    "    load_job.result()  # Waits for table load to complete.\n",
    "    print(\"Job finished.\")\n",
    "\n",
    "    destination_table = client.get_table(dataset_ref.table(bquery_table_name))\n",
    "    print(\"Loaded {} rows.\".format(destination_table.num_rows))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-latest.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "import xmltodict\n",
    "\n",
    "import untangle\n",
    "\n",
    "import numpy as np\n",
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "pd.options.display.max_rows = 9999\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from gcloud import storage\n",
    "from oauth2client.service_account import ServiceAccountCredentials\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "pd.options.display.max_columns = 999\n",
    "\n",
    "import plotly\n",
    "import plotly.graph_objs as go\n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot, offline\n",
    "offline.init_notebook_mode(connected=True)\n",
    "import plotly.express as px\n",
    "\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "%pylab inline\n",
    "pylab.rcParams['figure.figsize'] = (12, 12)\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "\n",
    "# For changes in .py\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import missingno as msno\n",
    "\n",
    "stats = ['skew', 'mad', 'kurt']\n",
    "\n",
    "\n",
    "import qgrid\n",
    "\n",
    "def qg(df):\n",
    "    return(qgrid.show_grid(df,show_toolbar=True, grid_options={'forceFitColumns': False}))\n",
    "\n",
    "import itertools\n",
    "pd.set_option('display.max_rows', 999)\n",
    "pd.set_option('display.max_columns', 999)\n",
    "pd.set_option('display.width', -1)\n",
    "pd.set_option('display.max_colwidth', 100)\n",
    "\n",
    "import requests\n",
    "import json\n",
    "import xmltodict\n",
    "import re\n",
    "import urllib.request as urllib2\n",
    "import io\n",
    "import yaml\n",
    "\n",
    "import math\n",
    "import pysal as ps\n",
    "\n",
    "# from pysal.esda.mapclassify import Quantiles, Equal_Interval, Fisher_Jenks\n",
    "\n",
    "\n",
    "\n",
    "import imageio\n",
    "\n",
    "\n",
    "from sklearn import preprocessing\n",
    "\n",
    "\n",
    "from google.cloud import bigquery\n",
    "import os\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"]=\"/mnt/AEB0CCA7B0CC777D/jlab/gabinete_sv_credentials.json\"\n",
    "\n",
    "from io import BytesIO\n",
    "from zipfile import ZipFile\n",
    "\n",
    "\n",
    "from datetime import date\n",
    "today = str(date.today()).replace('-','_')\n",
    "\n",
    "\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import os.path\n",
    "from os import path\n",
    "\n",
    "import wget\n",
    "\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rpy2.robjects.packages as rpackages\n",
    "from rpy2.robjects.packages import importr\n",
    "from rpy2.robjects import pandas2ri\n",
    "#install.packages(\"read.dbc\")\n",
    "utils = rpackages.importr('read.dbc')\n",
    "utils_package = importr(\"utils\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ftp://ftp.datasus.gov.br/dissemin/publicos/SIHSUS/200801_/Dados/SPSP1801.dbc\n",
      "ftp://ftp.datasus.gov.br/dissemin/publicos/SIHSUS/200801_/Dados/SPSP1802.dbc\n",
      "ftp://ftp.datasus.gov.br/dissemin/publicos/SIHSUS/200801_/Dados/SPSP1803.dbc\n",
      "ftp://ftp.datasus.gov.br/dissemin/publicos/SIHSUS/200801_/Dados/SPSP1804.dbc\n",
      "ftp://ftp.datasus.gov.br/dissemin/publicos/SIHSUS/200801_/Dados/SPSP1805.dbc\n",
      "ftp://ftp.datasus.gov.br/dissemin/publicos/SIHSUS/200801_/Dados/SPSP1806.dbc\n",
      "ftp://ftp.datasus.gov.br/dissemin/publicos/SIHSUS/200801_/Dados/SPSP1807.dbc\n",
      "ftp://ftp.datasus.gov.br/dissemin/publicos/SIHSUS/200801_/Dados/SPSP1808.dbc\n",
      "ftp://ftp.datasus.gov.br/dissemin/publicos/SIHSUS/200801_/Dados/SPSP1809.dbc\n",
      "ftp://ftp.datasus.gov.br/dissemin/publicos/SIHSUS/200801_/Dados/SPSP1810.dbc\n",
      "ftp://ftp.datasus.gov.br/dissemin/publicos/SIHSUS/200801_/Dados/SPSP1811.dbc\n",
      "ftp://ftp.datasus.gov.br/dissemin/publicos/SIHSUS/200801_/Dados/SPSP1812.dbc\n",
      "ftp://ftp.datasus.gov.br/dissemin/publicos/SIHSUS/200801_/Dados/SPSP1901.dbc\n",
      "ftp://ftp.datasus.gov.br/dissemin/publicos/SIHSUS/200801_/Dados/SPSP1902.dbc\n",
      "ftp://ftp.datasus.gov.br/dissemin/publicos/SIHSUS/200801_/Dados/SPSP1903.dbc\n",
      "ftp://ftp.datasus.gov.br/dissemin/publicos/SIHSUS/200801_/Dados/SPSP1904.dbc\n",
      "ftp://ftp.datasus.gov.br/dissemin/publicos/SIHSUS/200801_/Dados/SPSP1905.dbc\n",
      "ftp://ftp.datasus.gov.br/dissemin/publicos/SIHSUS/200801_/Dados/SPSP1906.dbc\n",
      "ftp://ftp.datasus.gov.br/dissemin/publicos/SIHSUS/200801_/Dados/SPSP1907.dbc\n",
      "ftp://ftp.datasus.gov.br/dissemin/publicos/SIHSUS/200801_/Dados/SPSP1908.dbc\n",
      "ftp://ftp.datasus.gov.br/dissemin/publicos/SIHSUS/200801_/Dados/SPSP1909.dbc\n",
      "Error on file: SPSP1910.dbc\n",
      "Error on file: SPSP1911.dbc\n",
      "Error on file: SPSP1912.dbc\n"
     ]
    }
   ],
   "source": [
    "anos   = ['18','19']\n",
    "months = ['01','02','03','04','05','06','07','08','09','10','11','12']\n",
    "\n",
    "\n",
    "for ano in anos:\n",
    "    for mes in months:\n",
    "        file = 'SPSP{}{}.dbc'.format(ano,mes)\n",
    "        url =     'ftp://ftp.datasus.gov.br/dissemin/publicos/SIHSUS/200801_/Dados/{}'.format(file)\n",
    "        try:\n",
    "            wget.download(url, out='../data/SIHSUS/dbc')\n",
    "            print(url)\n",
    "        except:\n",
    "            print('Error on file: {}'.format(file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_dbc = '../data/SIHSUS/dbc'\n",
    "path_parquet = '../data/SIHSUS/parquet'\n",
    "onlyfiles = [f for f in listdir(path_dbc) if isfile(join(path_dbc, f))]\n",
    "file_dbc = [\"{}/{}\".format(path_dbc,file) for file in onlyfiles]\n",
    "file_parquet = [\"{}/{}\".format(path_parquet,file.replace('.dbc','.parquet')) for file in onlyfiles]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/SIHSUS/parquet/SPSP1801.parquet\n",
      "loaded in r df\n",
      "saved .csv\n",
      "loaded in pandas df\n",
      "../data/SIHSUS/parquet/SPSP1802.parquet \n",
      "\n",
      "loaded in r df\n"
     ]
    }
   ],
   "source": [
    "for dbc_file, parquet_file in zip(file_dbc, file_parquet):\n",
    "    \n",
    "    if path.exists(parquet_file):\n",
    "        print(parquet_file,'\\n')\n",
    "        pass\n",
    "    else:\n",
    "        r_df = utils.read_dbc(dbc_file)\n",
    "        \n",
    "        print('loaded in R df')\n",
    "        \n",
    "        utils_package.write_csv(r_df, 'file_to_convert.csv', fileEncoding='utf-8')\n",
    "        \n",
    "        print('saved .csv')\n",
    "        \n",
    "        df = pd.read_csv('file_to_convert.csv')\n",
    "        print('loaded in pandas df')\n",
    "        \n",
    "        cols = ['SP_GESTOR','SP_UF','SP_AA','SP_MM','SP_CNES','SP_NAIH','SP_PROCREA','SP_DTINTER','SP_DTSAIDA','SP_NUM_PR','SP_TIPO','SP_CPFCGC','SP_ATOPROF',\n",
    "        'SP_TP_ATO','SP_QTD_ATO','SP_PTSP','SP_NF','SP_VALATO','SP_M_HOSP','SP_M_PAC','SP_DES_HOS','SP_DES_PAC','SP_COMPLEX','SP_FINANC','SP_CO_FAEC',\n",
    "        'SP_PF_CBO','SP_PF_DOC','SP_PJ_DOC','IN_TP_VAL','SEQUENCIA','REMESSA','SERV_CLA','SP_CIDPRI','SP_CIDSEC','SP_QT_PROC','SP_U_AIH']\n",
    "        df = df[cols]\n",
    "        \n",
    "        obj_cols = list(df.select_dtypes(include=['object']).columns)\n",
    "        for col in obj_cols:\n",
    "            df[col] = df[col].astype(str)\n",
    "\n",
    "        df.to_parquet(parquet_file, index=False)\n",
    "        \n",
    "        \n",
    "        \n",
    "        del r_df\n",
    "        gc.collect()\n",
    "        del df\n",
    "        gc.collect()\n",
    "        \n",
    "        \n",
    "        print(parquet_file, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
